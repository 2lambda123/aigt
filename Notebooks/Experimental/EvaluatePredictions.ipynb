{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = \"EvaluatePredictions\"\n",
    "\n",
    "# Update only this cell with your local parameters\n",
    "\n",
    "predictions_folder = \"ChildrensPredictionArrays\"\n",
    "ground_truth_file  = r\"ChildrensTestArrays\\segmentation-test.npy\"\n",
    "\n",
    "acceptable_margin_mm = 1.0\n",
    "mm_per_pixel = 1.0\n",
    "\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001,\n",
    "                  0.0008, 0.0006, 0.0004, 0.0002, 0.0001,\n",
    "                  0.00001, 0.000001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import scipy.ndimage\n",
    "\n",
    "from random import sample\n",
    "\n",
    "from evaluation_metrics import *\n",
    "\n",
    "from local_vars import root_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 predictions in d:\\Data\\ChildrensPredictionArrays\n",
      "Reading ground truth from:  d:\\Data\\ChildrensTestArrays\\segmentation-test.npy\n",
      "\n",
      "Found 1126 ground truth images\n"
     ]
    }
   ],
   "source": [
    "# Read data arrays\n",
    "\n",
    "predictions_folder_fullpath = os.path.join(root_folder, predictions_folder)\n",
    "\n",
    "predictions_file_list = [f for f in os.listdir(predictions_folder_fullpath) if f.endswith('.npy')]\n",
    "num_predictions = len(predictions_file_list)\n",
    "\n",
    "print(\"Found {} predictions in {}\".format(num_predictions, predictions_folder_fullpath))\n",
    "\n",
    "groundtruth_fullname = os.path.join(root_folder, ground_truth_file)\n",
    "\n",
    "print(\"Reading ground truth from:  {}\".format(groundtruth_fullname))\n",
    "\n",
    "groundtruth_data = np.load(groundtruth_fullname)\n",
    "num_groundtruth = groundtruth_data.shape[0]\n",
    "\n",
    "print(\"\\nFound {} ground truth images\".format(num_groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for file: q000_prediction.npy ...\n",
      "Calculating metrics for file: q001_prediction.npy ...\n",
      "Calculating metrics for file: q002_prediction.npy ...\n",
      "Calculating metrics for file: q003_prediction.npy ...\n",
      "Calculating metrics for file: q004_prediction.npy ...\n",
      "Calculating metrics for file: q005_prediction.npy ...\n",
      "Calculating metrics for file: q006_prediction.npy ...\n",
      "Calculating metrics for file: q007_prediction.npy ...\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "\n",
    "best_thresholds = np.zeros(num_predictions)\n",
    "best_tps = np.zeros(num_predictions)\n",
    "best_fps = np.zeros(num_predictions)\n",
    "aurocs = np.zeros(num_predictions)\n",
    "\n",
    "for i in range(num_predictions):\n",
    "    prediction_filename = predictions_file_list[i]\n",
    "    print(\"Calculating metrics for file: {} ...\".format(prediction_filename))\n",
    "    \n",
    "    prediction_fullname = os.path.join(predictions_folder_fullpath, prediction_filename)\n",
    "    prediction_data = np.load(prediction_fullname)\n",
    "    \n",
    "    true_positives, false_positives = compute_roc(\n",
    "        roc_thresholds, prediction_data, groundtruth_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    aurocs[i] = compute_auroc(true_positives, false_positives)\n",
    "    best_threshold_index = compute_goodness(roc_thresholds, true_positives, false_positives)\n",
    "    best_thresholds[i] = roc_thresholds[best_threshold_index]\n",
    "    best_tps[i] = true_positives[best_threshold_index]\n",
    "    best_fps[i] = false_positives[best_threshold_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction files\n",
      "q000_prediction.npy\n",
      "q001_prediction.npy\n",
      "q002_prediction.npy\n",
      "q003_prediction.npy\n",
      "q004_prediction.npy\n",
      "q005_prediction.npy\n",
      "q006_prediction.npy\n",
      "q007_prediction.npy\n",
      "\n",
      "AUROCs\n",
      "0.9483584510785631\n",
      "0.973459018468078\n",
      "0.9849062909599142\n",
      "0.9825783411538758\n",
      "0.9593933860490936\n",
      "0.9685175158124867\n",
      "0.9699881866361789\n",
      "0.9648691499566852\n",
      "\n",
      "Best thresholds\n",
      "0.0004\n",
      "0.0002\n",
      "0.002\n",
      "0.0006\n",
      "0.0002\n",
      "0.0006\n",
      "0.0001\n",
      "0.0002\n",
      "\n",
      "Best true positive rates\n",
      "0.3531049569929896\n",
      "0.4753890518570079\n",
      "0.5422612240839259\n",
      "0.5862128971312087\n",
      "0.619947297767613\n",
      "0.6490329637547855\n",
      "0.6771491075423856\n",
      "0.7069805598369214\n",
      "\n",
      "Best false positive rates\n",
      "0.0005872794060996966\n",
      "0.0012885191123075401\n",
      "0.0019281233384759843\n",
      "0.0025538976954483416\n",
      "0.003207549584028584\n",
      "0.003935305614246261\n",
      "0.004796133299248595\n",
      "0.005963559383634305\n"
     ]
    }
   ],
   "source": [
    "# Print metrics\n",
    "\n",
    "print(\"\\nPrediction files\")\n",
    "for i in range(num_predictions):\n",
    "    print(predictions_file_list[i])\n",
    "\n",
    "print(\"\\nAUROCs\")\n",
    "for i in range(num_predictions):\n",
    "    print(aurocs[i])\n",
    "\n",
    "print(\"\\nBest thresholds\")\n",
    "for i in range(num_predictions):\n",
    "    print(best_thresholds[i])\n",
    "\n",
    "print(\"\\nBest true positive rates\")\n",
    "for i in range(num_predictions):\n",
    "    print(true_positives[i])\n",
    "\n",
    "print(\"\\nBest false positive rates\")\n",
    "for i in range(num_predictions):\n",
    "    print(false_positives[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require([\"base/js/namespace\"],function(Jupyter) {\n",
       "    Jupyter.notebook.save_checkpoint();\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save notebook\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: d:\\Data\\LeaveOneOutNotebooks\\EvaluatePredictions_2019-10-14_13-24-15.html\n"
     ]
    }
   ],
   "source": [
    "# Archive notebook with unique filenames based on timestamps in one single HTML file.\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "notebook_fullpath = os.path.join(root_folder, \"LeaveOneOutNotebooks\")\n",
    "\n",
    "if not os.path.exists(notebook_fullpath):\n",
    "    os.makedirs(notebook_fullpath)\n",
    "    print(\"Creating folder: {}\".format(notebook_fullpath))\n",
    "notebook_file_name = notebook_name + \"_\" + timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebook_fullpath, notebook_file_name)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \"+ notebook_name +\" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
