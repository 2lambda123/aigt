{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l1\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "from local_vars import root_folder\n",
    "\n",
    "num_classes = 2\n",
    "ultrasound_size = 128\n",
    "\n",
    "notebook_save_folder = r\"LeaveOneOutNotebooks\"\n",
    "model_save_folder = r\"LeaveOneOutModels\"\n",
    "\n",
    "train_data_folder = r\"LeaveOneOutTrainArrays\"\n",
    "test_data_folder = r\"LeaveOneOutTestArrays\"\n",
    "childrens_test_data_folder = r\"ChildrensTestArrays\"\n",
    "\n",
    "# Augmentation parameters\n",
    "max_rotation_angle = 10\n",
    "\n",
    "# Model parameters\n",
    "filter_multiplier = 10\n",
    "\n",
    "# Learning parameters\n",
    "num_epochs = 30\n",
    "batch_size = 24\n",
    "max_learning_rate = 0.002\n",
    "min_learning_rate = 0.00001\n",
    "regularization_rate = 0.0001\n",
    "WCE_weights = np.array([0.01, 0.25])\n",
    "learning_rate_decay = (max_learning_rate - min_learning_rate) / num_epochs\n",
    "\n",
    "# Other parameters\n",
    "num_show = 2\n",
    "outList = [r\"q000\", r\"q001\", r\"q002\", r\"q003\", r\"q004\", r\"q005\", r\"q006\", r\"q007\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Importing\n",
    "def get_train_data(root_folder, train_data_folder, ultrasound_file, segmentation_file):\n",
    "    train_data_fullpath = os.path.join(root_folder, train_data_folder)\n",
    "\n",
    "    ultrasound_fullname = os.path.join(train_data_fullpath, ultrasound_file)\n",
    "    segmentation_fullname = os.path.join(train_data_fullpath, segmentation_file)\n",
    "\n",
    "    ultrasound_data = np.load(ultrasound_fullname)\n",
    "    segmentation_data = np.load(segmentation_fullname)\n",
    "\n",
    "    num_ultrasound = ultrasound_data.shape[0]\n",
    "    num_segmentation = segmentation_data.shape[0]\n",
    "    \n",
    "    return ultrasound_data, segmentation_data, num_ultrasound, num_segmentation, train_data_fullpath\n",
    "\n",
    "def get_test_data(root_folder, test_data_folder, test_ultrasound_file, test_segmentation_file):\n",
    "\n",
    "    test_data_fullpath = os.path.join(root_folder, test_data_folder)\n",
    "\n",
    "    test_ultrasound_fullname = os.path.join(test_data_fullpath, test_ultrasound_file)\n",
    "    test_segmentation_fullname = os.path.join(test_data_fullpath, test_segmentation_file)\n",
    "\n",
    "    test_ultrasound_data = np.load(test_ultrasound_fullname)\n",
    "    test_segmentation_data = np.load(test_segmentation_fullname)\n",
    "\n",
    "    num_test_ultrasound = test_ultrasound_data.shape[0]\n",
    "    num_test_segmentation = test_segmentation_data.shape[0]\n",
    "    \n",
    "    return test_ultrasound_data, test_segmentation_data, num_test_ultrasound, num_test_segmentation, test_data_fullpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ultrasound Segmentation Batch Generator Class\n",
    "import keras.utils\n",
    "import scipy.ndimage\n",
    "\n",
    "class UltrasoundSegmentationBatchGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 x_set,\n",
    "                 y_set,\n",
    "                 batch_size,\n",
    "                 image_dimensions=(ultrasound_size, ultrasound_size),\n",
    "                 shuffle=True,\n",
    "                 n_channels=1,\n",
    "                 n_classes=2):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_dimensions = image_dimensions\n",
    "        self.shuffle = shuffle\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.number_of_images = self.x.shape[0]\n",
    "        self.indexes = np.arange(self.number_of_images)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.number_of_images / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.number_of_images)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_indexes = self.indexes[index*self.batch_size : (index+1)*self.batch_size]\n",
    "        x = np.empty((self.batch_size, *self.image_dimensions, self.n_channels))\n",
    "        y = np.empty((self.batch_size, *self.image_dimensions))\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            flip_flag = np.random.randint(2)\n",
    "            if flip_flag == 1:\n",
    "                x[i,:,:,:] = np.flip(self.x[batch_indexes[i],:,:,:], axis=1)\n",
    "                y[i,:,:] = np.flip(self.y[batch_indexes[i],:,:], axis=1)\n",
    "            else:\n",
    "                x[i,:,:,:] = self.x[batch_indexes[i],:,:,:]\n",
    "                y[i,:,:] = self.y[batch_indexes[i],:,:]\n",
    "        \n",
    "        angle = np.random.randint(-max_rotation_angle, max_rotation_angle) \n",
    "        x_rot = scipy.ndimage.interpolation.rotate(x, angle, (1,2), False, mode=\"constant\", cval=0, order=0)\n",
    "        y_rot = scipy.ndimage.interpolation.rotate(y, angle, (1,2), False, mode=\"constant\", cval=0, order=0)\n",
    "        \n",
    "        x_rot = np.clip(x_rot, 0.0, 1.0)\n",
    "        y_rot = np.clip(y_rot, 0.0, 1.0)\n",
    "        \n",
    "        y_onehot = keras.utils.to_categorical(y_rot, self.n_classes)\n",
    "        \n",
    "        return x_rot, y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialateStack(segmentation_data, iterations):\n",
    "    \n",
    "    return np.array([scipy.ndimage.binary_dilation(y, iterations=iterations) for y in segmentation_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Net Model Construction\n",
    "from keras import backend as K\n",
    "\n",
    "def nvidia_unet(patch_size=ultrasound_size, num_classes=num_classes, regularization_rate=0.):\n",
    "    input_ = Input((patch_size, patch_size, 1))\n",
    "    skips = []\n",
    "    output = input_\n",
    "    \n",
    "    num_layers = int(np.floor(np.log2(patch_size)))\n",
    "    down_conv_kernel_sizes = np.zeros([num_layers], dtype=int)\n",
    "    down_filter_numbers = np.zeros([num_layers], dtype=int)\n",
    "    up_conv_kernel_sizes = np.zeros([num_layers], dtype=int)\n",
    "    up_filter_numbers = np.zeros([num_layers], dtype=int)\n",
    "    \n",
    "    for layer_index in range(num_layers):\n",
    "        down_conv_kernel_sizes[layer_index] = int(3)\n",
    "        down_filter_numbers[layer_index] = int( (layer_index + 1) * filter_multiplier + num_classes )\n",
    "        up_conv_kernel_sizes[layer_index] = int(4)\n",
    "        up_filter_numbers[layer_index] = int( (num_layers - layer_index - 1) * filter_multiplier + num_classes )\n",
    "    \n",
    "    for shape, filters in zip(down_conv_kernel_sizes, down_filter_numbers):\n",
    "        skips.append(output)\n",
    "        output = Conv2D(filters, (shape, shape), strides=2, padding=\"same\", activation=\"relu\", bias_regularizer=l1(regularization_rate))(output)\n",
    "    \n",
    "    for shape, filters in zip(up_conv_kernel_sizes, up_filter_numbers):\n",
    "        output = keras.layers.UpSampling2D()(output)\n",
    "        skip_output = skips.pop()\n",
    "        output = concatenate([output, skip_output], axis=3)\n",
    "        if filters != num_classes:\n",
    "            activation = \"relu\"\n",
    "            output = Conv2D(filters, (shape, shape), activation=\"relu\", padding=\"same\", bias_regularizer=l1(regularization_rate))(output)\n",
    "            output = BatchNormalization(momentum=.9)(output)\n",
    "        else:\n",
    "            activation = \"softmax\"\n",
    "            output = Conv2D(filters, (shape, shape), activation=\"softmax\", padding=\"same\", bias_regularizer=l1(regularization_rate))(output)\n",
    "    \n",
    "    assert len(skips) == 0\n",
    "    return Model([input_], [output])\n",
    "\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    \"\"\"\n",
    "    Dice = (2*|X & Y|)/ (|X|+ |Y|)\n",
    "         =  2*sum(|A*B|)/(sum(A^2)+sum(B^2))\n",
    "    ref: https://arxiv.org/pdf/1606.04797v1.pdf\n",
    "    \"\"\"\n",
    "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
    "    \n",
    "    return (2. * intersection + smooth) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n",
    "\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \"\"\"\n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################################\n",
      "Training network that leaves out q000\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9861342367671785 val loss: 0.0004754458190984137 val_dice: 0.9925244138354347\n",
      "\n",
      "Training started at: 2019-10-11 07:28:30.213931\n",
      "Training stopped at: 2019-10-11 07:33:21.688138\n",
      "Total training time: 0:04:51.474207\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q000_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q000_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_07-33-24.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q001\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9955722221306392 val loss: 0.0011215169943170622 val_dice: 0.9974862464836666\n",
      "\n",
      "Training started at: 2019-10-11 07:33:28.442798\n",
      "Training stopped at: 2019-10-11 07:38:16.180118\n",
      "Total training time: 0:04:47.737320\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q001_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q001_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_07-38-18.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q002\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9886364842716017 val loss: 0.0004984347860475904 val_dice: 0.994091294313732\n",
      "\n",
      "Training started at: 2019-10-11 07:38:23.261264\n",
      "Training stopped at: 2019-10-11 07:43:16.400200\n",
      "Total training time: 0:04:53.138936\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q002_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q002_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_07-43-19.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q003\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9884520669778188 val loss: 0.0007736292221428206 val_dice: 0.9939383169015249\n",
      "\n",
      "Training started at: 2019-10-11 07:43:25.800661\n",
      "Training stopped at: 2019-10-11 07:48:26.618634\n",
      "Total training time: 0:05:00.817973\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q003_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q003_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_07-48-29.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q004\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9900143990914027 val loss: 0.0004876272408485723 val_dice: 0.9946569999059042\n",
      "\n",
      "Training started at: 2019-10-11 07:48:37.499901\n",
      "Training stopped at: 2019-10-11 07:53:51.737883\n",
      "Total training time: 0:05:14.237982\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q004_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q004_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_07-53-55.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q005\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.987604308873415 val loss: 0.0005975551230221754 val_dice: 0.9935019910335541\n",
      "\n",
      "Training started at: 2019-10-11 07:54:04.802725\n",
      "Training stopped at: 2019-10-11 07:59:16.551163\n",
      "Total training time: 0:05:11.748438\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q005_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q005_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_07-59-20.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q006\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9840521931648254 val loss: 0.0006712475791573524 val_dice: 0.9916578094164531\n",
      "\n",
      "Training started at: 2019-10-11 07:59:31.885347\n",
      "Training stopped at: 2019-10-11 08:05:35.248187\n",
      "Total training time: 0:06:03.362840\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q006_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q006_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_08-05-38.h5\n",
      "\n",
      "#############################################\n",
      "Training network that leaves out q007\n",
      "#############################################\n",
      "\n",
      "val_acc: 0.9903012050522698 val loss: 0.0006599293087169321 val_dice: 0.9948874678876665\n",
      "\n",
      "Training started at: 2019-10-11 08:05:49.533224\n",
      "Training stopped at: 2019-10-11 08:10:41.307765\n",
      "Total training time: 0:04:51.774541\n",
      "\n",
      "Predictions saved to: c:\\Data\\LeaveOneOutTestArrays\\q007_prediction.npy\n",
      "\n",
      "Childrens predictions saved to: c:\\Data\\ChildrensTestArrays\\q007_prediction.npy\n",
      "\n",
      "Model saved to: c:\\Data\\LeaveOneOutModels\\model_2019-10-11_08-10-45.h5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for out in outList:\n",
    "    #\n",
    "    # Add in Children's Predictions at the same time (so we're using the same model for both)\n",
    "    #\n",
    "    ultrasound_file = out + r\"out_ultrasound.npy\"\n",
    "    segmentation_file = out + r\"out_segmentation.npy\"\n",
    "\n",
    "    # Get Leave Out One Filenames\n",
    "    test_ultrasound_file = out + r\"_ultrasound.npy\"\n",
    "    test_segmentation_file = out + r\"_segmentation.npy\"\n",
    "    test_prediction_file = out + r\"_prediction.npy\"\n",
    "    \n",
    "    # Get Childrens Test Filenames\n",
    "    childrens_test_ultrasound_file = r\"ultrasound-test.npy\"\n",
    "    childrens_test_segmentation_file = r\"segmentation-test.npy\"\n",
    "    childrens_test_prediction_file = out + r\"_prediction.npy\"\n",
    "    \n",
    "    print(\"#############################################\")\n",
    "    print(\"Training network that leaves out\", out)\n",
    "    print(\"#############################################\\n\")\n",
    "    \n",
    "    # Get Data\n",
    "    # Train\n",
    "    ultrasound_data, segmentation_data, num_ultrasound, num_segmentation, train_data_fullpath = get_train_data(root_folder, train_data_folder, ultrasound_file, segmentation_file)\n",
    "    # L-1-O Test\n",
    "    test_ultrasound_data, test_segmentation_data, num_test_ultrasound, num_test_segmentation, test_data_fullpath = get_test_data(root_folder, test_data_folder, test_ultrasound_file, test_segmentation_file)\n",
    "    # Childrens Test\n",
    "    childrens_test_ultrasound_data, childrens_test_segmentation_data, childrens_num_test_ultrasound, childrens_num_test_segmentation, childrens_test_data_fullpath = get_test_data(root_folder, childrens_test_data_folder, childrens_test_ultrasound_file, childrens_test_segmentation_file)\n",
    "    \n",
    "    # Prepare Dilated Outputs\n",
    "    width = 1\n",
    "    segmentation_dilated = dialateStack(segmentation_data[:, :, :, 0], width)\n",
    "    segmentation_dilated[:, :, :] = segmentation_data[:, :, :, 0]\n",
    "    \n",
    "    # Create Model\n",
    "    model = nvidia_unet(ultrasound_size, num_classes, regularization_rate)\n",
    "    model.compile(optimizer=keras.optimizers.adam(lr=max_learning_rate, decay=learning_rate_decay),\n",
    "              loss=[weighted_categorical_crossentropy(WCE_weights)],\n",
    "              metrics=[\"accuracy\", dice_coef])\n",
    "    \n",
    "    # Create Train and Test Generators\n",
    "    # Train\n",
    "    training_generator = UltrasoundSegmentationBatchGenerator(ultrasound_data, segmentation_dilated, batch_size)\n",
    "    # L-1-0 Test\n",
    "    test_generator = UltrasoundSegmentationBatchGenerator(test_ultrasound_data, test_segmentation_data[:, :, :, 0], batch_size)\n",
    "    # Childrens Test\n",
    "    childrens_test_generator = UltrasoundSegmentationBatchGenerator(childrens_test_ultrasound_data, childrens_test_segmentation_data[:, :, :, 0], batch_size)\n",
    "    \n",
    "    training_time_start = datetime.datetime.now()\n",
    "    \n",
    "    # Validate on ONLY L-1-O Test Data\n",
    "    training_log = model.fit_generator(training_generator,\n",
    "                                       validation_data=test_generator,\n",
    "                                       epochs=num_epochs,\n",
    "                                       verbose=0)\n",
    "        \n",
    "    training_time_stop = datetime.datetime.now()\n",
    "    \n",
    "    print(\"val_acc:\", training_log.history['val_acc'][-1], \"val loss:\", training_log.history['val_loss'][-1], \"val_dice:\", training_log.history['val_dice_coef'][-1])\n",
    "    print(\"\\nTraining started at: {}\".format(training_time_start))\n",
    "    print(\"Training stopped at: {}\".format(training_time_stop))\n",
    "    print(\"Total training time: {}\\n\".format(training_time_stop-training_time_start))\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    y_pred = model.predict(test_ultrasound_data)\n",
    "    childrens_y_pred = model.predict(childrens_test_ultrasound_data)\n",
    "    \n",
    "    # Saving L-1-O prediction for further evaluation\n",
    "    test_prediction_fullname = os.path.join(test_data_fullpath, test_prediction_file)\n",
    "    np.save(test_prediction_fullname, y_pred)\n",
    "    print(\"Predictions saved to: {}\\n\".format(test_prediction_fullname))\n",
    "\n",
    "    # Saving L-1-O prediction for further evaluation\n",
    "    childrens_test_prediction_fullname = os.path.join(childrens_test_data_fullpath, childrens_test_prediction_file)\n",
    "    np.save(childrens_test_prediction_fullname, childrens_y_pred)\n",
    "    print(\"Childrens predictions saved to: {}\\n\".format(childrens_test_prediction_fullname))\n",
    "    \n",
    "    # Archive model and notebook with unique filenames based on timestamps\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    saved_models_fullpath = os.path.join(root_folder, model_save_folder)\n",
    "    if not os.path.exists(saved_models_fullpath):\n",
    "        os.makedirs(saved_models_fullpath)\n",
    "        print(\"Creating folder: {}\".format(saved_models_fullpath))\n",
    "    model_file_name = \"model_\" + timestamp + \".h5\"\n",
    "    model_fullname = os.path.join(saved_models_fullpath, model_file_name)\n",
    "    model.save(model_fullname)\n",
    "    print(\"Model saved to: {}\\n\".format(model_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: c:\\Data\\LeaveOneOutNotebooks\\notebook_2019-10-11_08-10-53.html\n"
     ]
    }
   ],
   "source": [
    "# Save HTML copy of full Notebook.\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "saved_notebooks_fullpath = os.path.join(root_folder, notebook_save_folder)\n",
    "if not os.path.exists(saved_notebooks_fullpath):\n",
    "    os.makedirs(saved_notebooks_fullpath)\n",
    "    print(\"Creating folder: {}\".format(saved_notebooks_fullpath))\n",
    "\n",
    "notebook_file_name = \"notebook_\" + timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(saved_notebooks_fullpath, notebook_file_name)\n",
    "import time\n",
    "time.sleep(30)\n",
    "os.system(\"jupyter nbconvert --to html Segmentation2-QueensToChildrens --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
