{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = \"SegmentationEvaluation_Grouped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_vars import root_folder\n",
    "import os\n",
    "\n",
    "# Updating these names to reflect your data folders and files.\n",
    "\n",
    "notebook_fullpath = os.path.join(root_folder, \"LeaveOneOutNotebooks\")\n",
    "\n",
    "test_arrays_folder = \"LeaveOneOutTestArrays\"\n",
    "\n",
    "outList = [r\"q000\", r\"q001\", r\"q002\", r\"q003\", r\"q004\", r\"q005\", r\"q006\", r\"q007\"]\n",
    "\n",
    "acceptable_margin_mm = 1\n",
    "mm_per_pixel = 1\n",
    "\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001,\n",
    "                  0.0008, 0.0006, 0.0004, 0.0002, 0.0001,\n",
    "                  0.00001, 0.000001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import scipy.ndimage\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(groundtruth_fullname, prediction_fullname):\n",
    "    groundtruth_data = np.load(groundtruth_fullname)\n",
    "    prediction_data = np.load(prediction_fullname)\n",
    "\n",
    "    num_groundtruth = groundtruth_data.shape[0]\n",
    "    num_prediction = prediction_data.shape[0]\n",
    "\n",
    "    print(\"Found {} ground truth images and {} predictions\\n\".format(num_groundtruth, num_prediction))\n",
    "\n",
    "    if num_groundtruth != num_prediction:\n",
    "        print(\"Number of images should be equal!\")\n",
    "        raise\n",
    "    \n",
    "    return groundtruth_data, prediction_data, num_groundtruth, num_prediction\n",
    "\n",
    "def dilate_stack(segmentation_data, iterations):\n",
    "    return np.array([scipy.ndimage.binary_dilation(y, iterations=iterations) for y in segmentation_data])\n",
    "\n",
    "def dilate_ground(groundtruth_data, acceptable_margin_mm, mm_per_pixel):\n",
    "    acceptable_margin_pixel = int(acceptable_margin_mm / mm_per_pixel)\n",
    "    acceptable_region = dilate_stack(groundtruth_data[:, :, :, 0], acceptable_margin_pixel)\n",
    "    return acceptable_region\n",
    "\n",
    "def compute_regions(groundtruth_data, prediction_data, acceptable_region):\n",
    "    true_pos_prediction = np.minimum(groundtruth_data[:,:,:,0], prediction_data[:,:,:,1])\n",
    "    not_acceptable_region = 1 - acceptable_region\n",
    "    false_pos_prediction = np.minimum(not_acceptable_region, prediction_data[:, :, :, 1])\n",
    "    return true_pos_prediction, false_pos_prediction, not_acceptable_region\n",
    "\n",
    "def compute_prediction_amounts(groundtruth_data, not_acceptable_region, true_pos_prediction, false_pos_prediction):\n",
    "    fpp = np.sum(false_pos_prediction[:,:,:])\n",
    "    tna = np.sum(not_acceptable_region[:,:,:])\n",
    "    tpp = np.sum(true_pos_prediction)\n",
    "    tpa = np.sum(groundtruth_data[:,:,:,0])\n",
    "\n",
    "    print(\"Total false positive prediction amount per image: {:.2f}\".format(fpp / num_groundtruth))\n",
    "    print(\"Total true negative area per image:               {:.2f}\".format(tna / num_groundtruth))\n",
    "    print(\"  {:.2f}% of the true negative area was correctly predicted\".format((tna - fpp) / tna * 100))\n",
    "    print(\"\")\n",
    "    print(\"Total true positive prediction amount per image: {:.2f}\".format(tpp / num_groundtruth))\n",
    "    print(\"Total true positive area per image:              {:.2f}\".format(tpa / num_groundtruth))\n",
    "    print(\"  {:.2f}% of the true positive area was correctly predicted\\n\".format(tpp / tpa * 100))\n",
    "\n",
    "def compute_roc(roc_thresholds, prediction_data, groundtruth_data, acceptable_margin_mm, mm_per_pixel):\n",
    "    false_positives = np.zeros(len(roc_thresholds))\n",
    "    true_positives = np.zeros(len(roc_thresholds))\n",
    "\n",
    "    for i in range(len(roc_thresholds)):\n",
    "        threshold = roc_thresholds[i]\n",
    "        prediction_thresholded = np.copy(prediction_data)\n",
    "        prediction_thresholded[prediction_thresholded >= threshold] = 1.0\n",
    "        prediction_thresholded[prediction_thresholded < threshold] = 0.0\n",
    "        metrics = evaluation_metrics.compute_evaluation_metrics(\n",
    "            prediction_thresholded, groundtruth_data, acceptable_margin_mm=acceptable_margin_mm, mm_per_pixel=mm_per_pixel)\n",
    "        true_negative_area_perc = metrics[evaluation_metrics.TRUE_NEGATIVE_AREA_PERCENT]\n",
    "        false_positives[i] = (100 - true_negative_area_perc) / 100.0\n",
    "        true_positives[i] = metrics[evaluation_metrics.TRUE_POSITIVE_AREA_PERCENT] / 100.0\n",
    "    \n",
    "    return true_positives, false_positives\n",
    "\n",
    "# Goodness is defined as distance from the diagonal of the ROC curve\n",
    "def compute_goodness(roc_thresholds, true_positives, false_positives):\n",
    "    goodnesses = np.zeros(len(roc_thresholds))\n",
    "    for i in range(len(roc_thresholds)):\n",
    "        crossprod = np.cross((1.0, 1.0), (false_positives[i], true_positives[i]))\n",
    "        goodnesses[i] = np.linalg.norm(crossprod)/np.linalg.norm([1.0, 1.0])\n",
    "\n",
    "    best_threshold_index = np.argmax(goodnesses)\n",
    "    print(\"Best threshold:           {}\".format(roc_thresholds[best_threshold_index]))\n",
    "    print(\"Best true positive rate:  {}\".format(true_positives[best_threshold_index]))\n",
    "    print(\"Best false positive rate: {}\\n\".format(false_positives[best_threshold_index]))\n",
    "\n",
    "def compute_AUC(true_positives, false_positives):\n",
    "    area = 0.0\n",
    "\n",
    "    fps = np.zeros(len(false_positives) * 2)\n",
    "    tps = np.zeros(len(false_positives) * 2)\n",
    "\n",
    "    for i in range(len(false_positives)):\n",
    "        fps[i*2] = false_positives[i]\n",
    "        tps[i*2] = true_positives[i]\n",
    "        if i == len(false_positives) - 1:\n",
    "            fps[i*2+1] = 1.0\n",
    "            tps[i*2+1] = true_positives[i]\n",
    "            area = area + (1.0 - false_positives[i]) * true_positives[i]\n",
    "        else:\n",
    "            fps[i*2+1] = false_positives[i+1]\n",
    "            tps[i*2+1] = true_positives[i]\n",
    "            area = area + (false_positives[i+1] - false_positives[i]) * true_positives[i]\n",
    "\n",
    "    print(\"AUC = {}\\n\".format(area))\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Assessing Predictions on model that left out q000\n",
      "#######################################################\n",
      "\n",
      "Found 523 ground truth images and 523 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 91.65\n",
      "Total true negative area per image:               16281.60\n",
      "  99.44% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 36.11\n",
      "Total true positive area per image:              45.86\n",
      "  78.74% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.001\n",
      "Best true positive rate:  0.9790685068590251\n",
      "Best false positive rate: 0.030992528957950752\n",
      "\n",
      "AUC = 0.9930328046429661\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q001\n",
      "#######################################################\n",
      "\n",
      "Found 355 ground truth images and 355 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 45.36\n",
      "Total true negative area per image:               16291.24\n",
      "  99.72% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 21.65\n",
      "Total true positive area per image:              41.02\n",
      "  52.78% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.0004\n",
      "Best true positive rate:  0.960030217704828\n",
      "Best false positive rate: 0.0512825522747039\n",
      "\n",
      "AUC = 0.9873183032560494\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q002\n",
      "#######################################################\n",
      "\n",
      "Found 477 ground truth images and 477 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 103.17\n",
      "Total true negative area per image:               16282.33\n",
      "  99.37% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 35.85\n",
      "Total true positive area per image:              47.14\n",
      "  76.05% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.004\n",
      "Best true positive rate:  0.981054036024016\n",
      "Best false positive rate: 0.03195487898932242\n",
      "\n",
      "AUC = 0.995210620476392\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q003\n",
      "#######################################################\n",
      "\n",
      "Found 453 ground truth images and 453 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 96.48\n",
      "Total true negative area per image:               16246.82\n",
      "  99.41% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 48.37\n",
      "Total true positive area per image:              65.77\n",
      "  73.54% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.001\n",
      "Best true positive rate:  0.9741903003859708\n",
      "Best false positive rate: 0.03566409517853103\n",
      "\n",
      "AUC = 0.9915673229589024\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q004\n",
      "#######################################################\n",
      "\n",
      "Found 289 ground truth images and 289 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 86.31\n",
      "Total true negative area per image:               16292.19\n",
      "  99.47% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 32.21\n",
      "Total true positive area per image:              42.71\n",
      "  75.42% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.001\n",
      "Best true positive rate:  0.9760168530222006\n",
      "Best false positive rate: 0.03303874190231085\n",
      "\n",
      "AUC = 0.9919770374952523\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q005\n",
      "#######################################################\n",
      "\n",
      "Found 387 ground truth images and 387 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 116.21\n",
      "Total true negative area per image:               16250.99\n",
      "  99.28% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 45.39\n",
      "Total true positive area per image:              58.64\n",
      "  77.41% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.004\n",
      "Best true positive rate:  0.9744866484533357\n",
      "Best false positive rate: 0.03348108022503567\n",
      "\n",
      "AUC = 0.9928248864127887\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q006\n",
      "#######################################################\n",
      "\n",
      "Found 360 ground truth images and 360 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 108.71\n",
      "Total true negative area per image:               16241.85\n",
      "  99.33% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 53.00\n",
      "Total true positive area per image:              69.35\n",
      "  76.43% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.002\n",
      "Best true positive rate:  0.9715601842579611\n",
      "Best false positive rate: 0.03510273957960109\n",
      "\n",
      "AUC = 0.9900191618507441\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q007\n",
      "#######################################################\n",
      "\n",
      "Found 446 ground truth images and 446 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 74.73\n",
      "Total true negative area per image:               16239.59\n",
      "  99.54% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 49.80\n",
      "Total true positive area per image:              67.61\n",
      "  73.66% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.0006\n",
      "Best true positive rate:  0.9789082708761689\n",
      "Best false positive rate: 0.03337412294623519\n",
      "\n",
      "AUC = 0.9932286694460986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This runs the bulk of the computation of metrics.\n",
    "for out in outList:\n",
    "    \n",
    "    print(\"#######################################################\")\n",
    "    print(\"Assessing Predictions on model that left out\", out)\n",
    "    print(\"#######################################################\\n\")\n",
    "\n",
    "    # Use these for the Leave One Out Data Set.\n",
    "    test_arrays_fullpath = os.path.join(root_folder, test_arrays_folder)\n",
    "    groundtruth_fullname = os.path.join(test_arrays_fullpath, out + r\"_segmentation.npy\")\n",
    "    prediction_fullname  = os.path.join(test_arrays_fullpath, out + r\"_prediction.npy\")\n",
    "\n",
    "    # Use these for the Children's Data Set.\n",
    "    #groundtruth_fullname = r\"c:\\Data\\ChildrensTestArrays\\segmentation-test.npy\"\n",
    "    #prediction_fullname=r\"c:\\Data\\ChildrensTestArrays\\\\\" + out + r\"_prediction.npy\"\n",
    "\n",
    "    # Prepare data\n",
    "    \n",
    "    groundtruth_data, prediction_data, num_groundtruth, num_prediction =\\\n",
    "        read_data(groundtruth_fullname, prediction_fullname)\n",
    "    acceptable_region = dilate_ground(groundtruth_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    true_pos_prediction, false_pos_prediction, not_acceptable_region =\\\n",
    "        compute_regions(groundtruth_data, prediction_data, acceptable_region)\n",
    "    \n",
    "    # Compute metrics.\n",
    "    compute_prediction_amounts(groundtruth_data, not_acceptable_region, true_pos_prediction, false_pos_prediction)\n",
    "    true_positives, false_positives = compute_roc(roc_thresholds, prediction_data, groundtruth_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    compute_goodness(roc_thresholds, true_positives, false_positives)\n",
    "    area = compute_AUC(true_positives, false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require([\"base/js/namespace\"],function(Jupyter) {\n",
       "    Jupyter.notebook.save_checkpoint();\n",
       "});\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save notebook\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: d:\\Data\\LeaveOneOutNotebooks\\SegmentationEvaluation_Grouped_2019-10-13_14-47-38.html\n"
     ]
    }
   ],
   "source": [
    "# Archive notebook with unique filenames based on timestamps in one single HTML file.\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "if not os.path.exists(notebook_fullpath):\n",
    "    os.makedirs(notebook_fullpath)\n",
    "    print(\"Creating folder: {}\".format(notebook_fullpath))\n",
    "notebook_file_name = notebook_name + \"_\" + timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebook_fullpath, notebook_file_name)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \"+ notebook_name +\" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
