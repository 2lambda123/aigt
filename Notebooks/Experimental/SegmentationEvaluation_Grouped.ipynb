{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os \n",
    "import scipy.ndimage\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import evaluation_metrics\n",
    "\n",
    "# Updating these names to reflect the location of your data.\n",
    "notebook_fullpath = r\"c:\\Data\\LeaveOneOutNotebooks\"\n",
    "\n",
    "# Update this to change the file prefix of the saved HTML file.\n",
    "notebook_name = \"SegmentationEvaluation_Grouped\"\n",
    "\n",
    "acceptable_margin_mm = 1\n",
    "mm_per_pixel = 1\n",
    "\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001,\n",
    "                  0.0008, 0.0006, 0.0004, 0.0002, 0.0001,\n",
    "                  0.00001, 0.000001]\n",
    "\n",
    "# Change these to match your file prefixes\n",
    "outList = [r\"q000\", r\"q001\", r\"q002\", r\"q003\", r\"q004\", r\"q005\", r\"q006\", r\"q007\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(groundtruth_fullname, prediction_fullname):\n",
    "    groundtruth_data = np.load(groundtruth_fullname)\n",
    "    prediction_data = np.load(prediction_fullname)\n",
    "\n",
    "    num_groundtruth = groundtruth_data.shape[0]\n",
    "    num_prediction = prediction_data.shape[0]\n",
    "\n",
    "    print(\"Found {} ground truth images and {} predictions\\n\".format(num_groundtruth, num_prediction))\n",
    "\n",
    "    if num_groundtruth != num_prediction:\n",
    "        print(\"Number of images should be equal!\")\n",
    "        raise\n",
    "    \n",
    "    return groundtruth_data, prediction_data, num_groundtruth, num_prediction\n",
    "\n",
    "def dilate_stack(segmentation_data, iterations):\n",
    "    \n",
    "    return np.array([scipy.ndimage.binary_dilation(y, iterations=iterations) for y in segmentation_data])\n",
    "\n",
    "def dilate_ground(groundtruth_data, acceptable_margin_mm, mm_per_pixel):\n",
    "    acceptable_margin_pixel = int(acceptable_margin_mm / mm_per_pixel)\n",
    "    acceptable_region = dilate_stack(groundtruth_data[:, :, :, 0], acceptable_margin_pixel)\n",
    "    \n",
    "    return acceptable_region\n",
    "\n",
    "def compute_regions(groundtruth_data, prediction_data, acceptable_region):\n",
    "    true_pos_prediction = np.minimum(groundtruth_data[:,:,:,0], prediction_data[:,:,:,1])\n",
    "    not_acceptable_region = 1 - acceptable_region\n",
    "    false_pos_prediction = np.minimum(not_acceptable_region, prediction_data[:, :, :, 1])\n",
    "    \n",
    "    return true_pos_prediction, false_pos_prediction, not_acceptable_region\n",
    "\n",
    "def compute_prediction_amounts(groundtruth_data, not_acceptable_region, true_pos_prediction, false_pos_prediction):\n",
    "    fpp = np.sum(false_pos_prediction[:,:,:])\n",
    "    tna = np.sum(not_acceptable_region[:,:,:])\n",
    "    tpp = np.sum(true_pos_prediction)\n",
    "    tpa = np.sum(groundtruth_data[:,:,:,0])\n",
    "\n",
    "    print(\"Total false positive prediction amount per image: {:.2f}\".format(fpp / num_groundtruth))\n",
    "    print(\"Total true negative area per image:               {:.2f}\".format(tna / num_groundtruth))\n",
    "    print(\"  {:.2f}% of the true negative area was correctly predicted\".format((tna - fpp) / tna * 100))\n",
    "    print(\"\")\n",
    "    print(\"Total true positive prediction amount per image: {:.2f}\".format(tpp / num_groundtruth))\n",
    "    print(\"Total true positive area per image:              {:.2f}\".format(tpa / num_groundtruth))\n",
    "    print(\"  {:.2f}% of the true positive area was correctly predicted\\n\".format(tpp / tpa * 100))\n",
    "\n",
    "def compute_roc(roc_thresholds, prediction_data, groundtruth_data, acceptable_margin_mm, mm_per_pixel):\n",
    "    false_positives = np.zeros(len(roc_thresholds))\n",
    "    true_positives = np.zeros(len(roc_thresholds))\n",
    "\n",
    "    for i in range(len(roc_thresholds)):\n",
    "        threshold = roc_thresholds[i]\n",
    "        prediction_thresholded = np.copy(prediction_data)\n",
    "        prediction_thresholded[prediction_thresholded >= threshold] = 1.0\n",
    "        prediction_thresholded[prediction_thresholded < threshold] = 0.0\n",
    "        metrics = evaluation_metrics.compute_evaluation_metrics(\n",
    "            prediction_thresholded, groundtruth_data, acceptable_margin_mm=acceptable_margin_mm, mm_per_pixel=mm_per_pixel)\n",
    "        true_negative_area_perc = metrics[evaluation_metrics.TRUE_NEGATIVE_AREA_PERCENT]\n",
    "        false_positives[i] = (100 - true_negative_area_perc) / 100.0\n",
    "        true_positives[i] = metrics[evaluation_metrics.TRUE_POSITIVE_AREA_PERCENT] / 100.0\n",
    "    \n",
    "    return true_positives, false_positives\n",
    "\n",
    "# Goodness is defined as distance from the diagonal of the ROC curve\n",
    "def compute_goodness(roc_thresholds, true_positives, false_positives):\n",
    "    goodnesses = np.zeros(len(roc_thresholds))\n",
    "    for i in range(len(roc_thresholds)):\n",
    "        crossprod = np.cross((1.0, 1.0), (false_positives[i], true_positives[i]))\n",
    "        goodnesses[i] = np.linalg.norm(crossprod)/np.linalg.norm([1.0, 1.0])\n",
    "\n",
    "    best_threshold_index = np.argmax(goodnesses)\n",
    "    print(\"Best threshold:           {}\".format(roc_thresholds[best_threshold_index]))\n",
    "    print(\"Best true positive rate:  {}\".format(true_positives[best_threshold_index]))\n",
    "    print(\"Best false positive rate: {}\\n\".format(false_positives[best_threshold_index]))\n",
    "\n",
    "def compute_AUC(true_positives, false_positives):\n",
    "    area = 0.0\n",
    "\n",
    "    fps = np.zeros(len(false_positives) * 2)\n",
    "    tps = np.zeros(len(false_positives) * 2)\n",
    "\n",
    "    for i in range(len(false_positives)):\n",
    "        fps[i*2] = false_positives[i]\n",
    "        tps[i*2] = true_positives[i]\n",
    "        if i == len(false_positives) - 1:\n",
    "            fps[i*2+1] = 1.0\n",
    "            tps[i*2+1] = true_positives[i]\n",
    "            area = area + (1.0 - false_positives[i]) * true_positives[i]\n",
    "        else:\n",
    "            fps[i*2+1] = false_positives[i+1]\n",
    "            tps[i*2+1] = true_positives[i]\n",
    "            area = area + (false_positives[i+1] - false_positives[i]) * true_positives[i]\n",
    "\n",
    "    print(\"AUC = {}\\n\".format(area))\n",
    "    \n",
    "    return area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######################################################\n",
      "Assessing Predictions on model that left out q000\n",
      "#######################################################\n",
      "\n",
      "Found 523 ground truth images and 523 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 195.41\n",
      "Total true negative area per image:               16281.60\n",
      "  98.80% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 40.32\n",
      "Total true positive area per image:              45.86\n",
      "  87.93% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.08\n",
      "Best true positive rate:  0.9856565066922404\n",
      "Best false positive rate: 0.025572858572556357\n",
      "\n",
      "AUC = 0.9956167428948614\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q001\n",
      "#######################################################\n",
      "\n",
      "Found 355 ground truth images and 355 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 96.74\n",
      "Total true negative area per image:               16291.24\n",
      "  99.41% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 19.26\n",
      "Total true positive area per image:              41.02\n",
      "  46.95% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.002\n",
      "Best true positive rate:  0.87954124029943\n",
      "Best false positive rate: 0.07533764798846349\n",
      "\n",
      "AUC = 0.9449540044859712\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q002\n",
      "#######################################################\n",
      "\n",
      "Found 477 ground truth images and 477 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 174.72\n",
      "Total true negative area per image:               16282.33\n",
      "  98.93% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 39.21\n",
      "Total true positive area per image:              47.14\n",
      "  83.18% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.04\n",
      "Best true positive rate:  0.9838114298421171\n",
      "Best false positive rate: 0.02981316317031627\n",
      "\n",
      "AUC = 0.9952404073429867\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q003\n",
      "#######################################################\n",
      "\n",
      "Found 453 ground truth images and 453 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 171.44\n",
      "Total true negative area per image:               16246.82\n",
      "  98.94% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 50.47\n",
      "Total true positive area per image:              65.77\n",
      "  76.74% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.04\n",
      "Best true positive rate:  0.9780500083906696\n",
      "Best false positive rate: 0.03293698710469613\n",
      "\n",
      "AUC = 0.9944478641345715\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q004\n",
      "#######################################################\n",
      "\n",
      "Found 289 ground truth images and 289 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 126.28\n",
      "Total true negative area per image:               16292.19\n",
      "  99.22% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 32.75\n",
      "Total true positive area per image:              42.71\n",
      "  76.69% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.01\n",
      "Best true positive rate:  0.9807972775887215\n",
      "Best false positive rate: 0.039754551505572236\n",
      "\n",
      "AUC = 0.994344930307061\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q005\n",
      "#######################################################\n",
      "\n",
      "Found 387 ground truth images and 387 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 257.69\n",
      "Total true negative area per image:               16250.99\n",
      "  98.41% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 49.42\n",
      "Total true positive area per image:              58.64\n",
      "  84.28% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.1\n",
      "Best true positive rate:  0.9833876795628801\n",
      "Best false positive rate: 0.03265870945029945\n",
      "\n",
      "AUC = 0.9942889250348549\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q006\n",
      "#######################################################\n",
      "\n",
      "Found 360 ground truth images and 360 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 215.94\n",
      "Total true negative area per image:               16241.85\n",
      "  98.67% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 57.95\n",
      "Total true positive area per image:              69.35\n",
      "  83.57% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.04\n",
      "Best true positive rate:  0.982375325455638\n",
      "Best false positive rate: 0.03624741643884576\n",
      "\n",
      "AUC = 0.9933009554488449\n",
      "\n",
      "#######################################################\n",
      "Assessing Predictions on model that left out q007\n",
      "#######################################################\n",
      "\n",
      "Found 446 ground truth images and 446 predictions\n",
      "\n",
      "Total false positive prediction amount per image: 135.47\n",
      "Total true negative area per image:               16239.59\n",
      "  99.17% of the true negative area was correctly predicted\n",
      "\n",
      "Total true positive prediction amount per image: 51.95\n",
      "Total true positive area per image:              67.61\n",
      "  76.83% of the true positive area was correctly predicted\n",
      "\n",
      "Best threshold:           0.02\n",
      "Best true positive rate:  0.9857398686741393\n",
      "Best false positive rate: 0.03298891583804561\n",
      "\n",
      "AUC = 0.9956328077403266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This runs the bulk of the computation of metrics.\n",
    "for out in outList:\n",
    "    \n",
    "    print(\"#######################################################\")\n",
    "    print(\"Assessing Predictions on model that left out\", out)\n",
    "    print(\"#######################################################\\n\")\n",
    "\n",
    "    # Use these for the Leave One Out Data Set.\n",
    "    groundtruth_fullname = r\"c:\\Data\\LeaveOneOutTestArrays\\\\\" + out + r\"_segmentation.npy\"\n",
    "    prediction_fullname=r\"c:\\Data\\LeaveOneOutTestArrays\\\\\" + out + r\"_prediction.npy\"\n",
    "\n",
    "    # Use these for the Children's Data Set.\n",
    "    #groundtruth_fullname = r\"c:\\Data\\ChildrensTestArrays\\segmentation-test.npy\"\n",
    "    #prediction_fullname=r\"c:\\Data\\ChildrensTestArrays\\\\\" + out + r\"_prediction.npy\"\n",
    "\n",
    "    # Get Data.\n",
    "    groundtruth_data, prediction_data, num_groundtruth, num_prediction = read_data(groundtruth_fullname, prediction_fullname)\n",
    "    # Dilate Data.\n",
    "    acceptable_region = dilate_ground(groundtruth_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    true_pos_prediction, false_pos_prediction, not_acceptable_region = compute_regions(groundtruth_data, prediction_data, acceptable_region)\n",
    "    # Compute Predictive Metrics.\n",
    "    compute_prediction_amounts(groundtruth_data, not_acceptable_region, true_pos_prediction, false_pos_prediction)\n",
    "    # Compute ROC Curve Data.\n",
    "    true_positives, false_positives = compute_roc(roc_thresholds, prediction_data, groundtruth_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    # Compute Goodness.\n",
    "    compute_goodness(roc_thresholds, true_positives, false_positives)\n",
    "    # Compute AUC.\n",
    "    area = compute_AUC(true_positives, false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook saved to: c:\\Data\\LeaveOneOutNotebooks\\SegmentationEvaluation_Grouped_2019-10-11_14-57-34.html\n"
     ]
    }
   ],
   "source": [
    "# Archive notebook with unique filenames based on timestamps in one single HTML file.\n",
    "timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "if not os.path.exists(notebook_fullpath):\n",
    "    os.makedirs(notebook_fullpath)\n",
    "    print(\"Creating folder: {}\".format(notebook_fullpath))\n",
    "notebook_file_name = notebook_name + \"_\" + timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebook_fullpath, notebook_file_name)\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \"+ notebook_name +\" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
