{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_notebook_name = \"SagittalSpineSegmentationStudy\"\n",
    "\n",
    "# Update this folder name for your computer\n",
    "\n",
    "local_data_folder = r\"d:\\Data\\SagittalSpineSegmentationStudy\"\n",
    "overwrite_existing_data_files = False\n",
    "\n",
    "# All results and output will be archived with this timestamp\n",
    "\n",
    "import datetime\n",
    "save_timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "print(\"Save timestamp: {}\".format(save_timestamp))\n",
    "\n",
    "# Learning parameters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "ultrasound_size = 128\n",
    "num_classes = 2\n",
    "num_epochs = 30\n",
    "batch_size = 24\n",
    "max_learning_rate = 0.02\n",
    "min_learning_rate = 0.00001\n",
    "regularization_rate = 0.0001\n",
    "WCE_weights = np.array([0.05, 0.95])\n",
    "learning_rate_decay = (max_learning_rate - min_learning_rate) / num_epochs\n",
    "\n",
    "# Training data augmentation parameters\n",
    "\n",
    "max_shift_factor = 0.12\n",
    "max_rotation_angle = 10\n",
    "max_zoom_factor = 1.1\n",
    "min_zoom_factor = 0.9\n",
    "\n",
    "# Evaluation parameters\n",
    "\n",
    "acceptable_margin_mm = 0\n",
    "mm_per_pixel = 1\n",
    "\n",
    "roc_thresholds = [0.9, 0.8, 0.7, 0.65, 0.6, 0.55, 0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.15, 0.1,\n",
    "                  0.08, 0.06, 0.04, 0.02, 0.01,\n",
    "                  0.008, 0.006, 0.004, 0.002, 0.001,\n",
    "                  0.0008, 0.0006, 0.0004, 0.0002, 0.0001]\n",
    "\n",
    "# roc_thresholds = [0.8, 0.6, 0.4,  0.2, 0.1, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import girder_client\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "import ultrasound_batch_generator as generator\n",
    "import sagittal_spine_segmentation_unet as unet\n",
    "import evaluation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what data to download\n",
    "\n",
    "girder_api_url = \"https://pocus.cs.queensu.ca/api/v1\"\n",
    "\n",
    "training_ultrasound_ids = [\n",
    "    \"5da9e5c0d9e6a3be02d012b4\",\n",
    "    \"5da9e5c7d9e6a3be02d012c6\",\n",
    "    \"5da9e5c2d9e6a3be02d012b7\",\n",
    "    \"5da9e5c3d9e6a3be02d012ba\",\n",
    "    \"5da9e5c8d9e6a3be02d012c9\",\n",
    "    \"5da9e5c5d9e6a3be02d012c0\",\n",
    "    \"5da9e5c6d9e6a3be02d012c3\",\n",
    "    \"5da9e5c4d9e6a3be02d012bd\"\n",
    "]\n",
    "\n",
    "training_ultrasound_filenames = [\n",
    "    \"q000_ultrasound.npy\",\n",
    "    \"q001_ultrasound.npy\",\n",
    "    \"q002_ultrasound.npy\",\n",
    "    \"q003_ultrasound.npy\",\n",
    "    \"q004_ultrasound.npy\",\n",
    "    \"q005_ultrasound.npy\",\n",
    "    \"q006_ultrasound.npy\",\n",
    "    \"q007_ultrasound.npy\"\n",
    "]\n",
    "\n",
    "training_segmentation_ids = [\n",
    "    \"5da9e5c8d9e6a3be02d012cc\",\n",
    "    \"5da9e5ccd9e6a3be02d012de\",\n",
    "    \"5da9e5c9d9e6a3be02d012cf\",\n",
    "    \"5da9e5cad9e6a3be02d012d2\",\n",
    "    \"5da9e5cdd9e6a3be02d012e1\",\n",
    "    \"5da9e5cbd9e6a3be02d012d8\",\n",
    "    \"5da9e5cbd9e6a3be02d012db\",\n",
    "    \"5da9e5cad9e6a3be02d012d5\"\n",
    "]\n",
    "\n",
    "training_segmentation_filenames = [\n",
    "    \"q000_segmentation.npy\",\n",
    "    \"q001_segmentation.npy\",\n",
    "    \"q002_segmentation.npy\",\n",
    "    \"q003_segmentation.npy\",\n",
    "    \"q004_segmentation.npy\",\n",
    "    \"q005_segmentation.npy\",\n",
    "    \"q006_segmentation.npy\",\n",
    "    \"q007_segmentation.npy\"\n",
    "]\n",
    "\n",
    "testing_ultrasound_filename = \"ultrasound-test.npy\"\n",
    "testing_ultrasound_id = \"5daa85edd9e6a3be02d012e7\"\n",
    "testing_segmentation_filename = \"segmentation-test.npy\"\n",
    "testing_segmentation_id = \"5daa85e7d9e6a3be02d012e4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These subfolders will be created/populated in the data folder\n",
    "\n",
    "data_arrays_folder    = \"DataArrays\"\n",
    "notebooks_save_folder = \"SavedNotebooks\"\n",
    "results_save_folder   = \"SavedResults\"\n",
    "models_save_folder    = \"SavedModels\"\n",
    "val_data_folder       = \"PredictionsValidation\"\n",
    "test_data_folder      = \"PredictionsTest\"\n",
    "\n",
    "data_arrays_fullpath = os.path.join(local_data_folder, data_arrays_folder)\n",
    "notebooks_save_fullpath = os.path.join(local_data_folder, notebooks_save_folder)\n",
    "results_save_fullpath = os.path.join(local_data_folder, results_save_folder)\n",
    "models_save_fullpath = os.path.join(local_data_folder, models_save_folder)\n",
    "val_data_fullpath = os.path.join(local_data_folder, val_data_folder)\n",
    "test_data_fullpath = os.path.join(local_data_folder, test_data_folder)\n",
    "\n",
    "if not os.path.exists(data_arrays_fullpath):\n",
    "    os.makedirs(data_arrays_fullpath)\n",
    "    print(\"Created folder: {}\".format(data_arrays_fullpath))\n",
    "\n",
    "if not os.path.exists(notebooks_save_fullpath):\n",
    "    os.makedirs(notebooks_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(notebooks_save_fullpath))\n",
    "\n",
    "if not os.path.exists(results_save_fullpath):\n",
    "    os.makedirs(results_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(results_save_fullpath))\n",
    "\n",
    "if not os.path.exists(models_save_fullpath):\n",
    "    os.makedirs(models_save_fullpath)\n",
    "    print(\"Created folder: {}\".format(models_save_fullpath))\n",
    "\n",
    "if not os.path.exists(val_data_fullpath):\n",
    "    os.makedirs(val_data_fullpath)\n",
    "    print(\"Created folder: {}\".format(val_data_fullpath))\n",
    "\n",
    "if not os.path.exists(test_data_fullpath):\n",
    "    os.makedirs(test_data_fullpath)\n",
    "    print(\"Created folder: {}\".format(test_data_fullpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading training files ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f556b5e9f4593be93460e01a5b162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total download time: 0:00:00.015959\n"
     ]
    }
   ],
   "source": [
    "# Download data from Girder\n",
    "\n",
    "time_download_start = datetime.datetime.now()\n",
    "\n",
    "print(\"Downloading training files ...\")\n",
    "\n",
    "n_files = len(training_ultrasound_ids)\n",
    "\n",
    "f = IntProgress(min=0, max=n_files*2)\n",
    "display(f)\n",
    "\n",
    "gclient = girder_client.GirderClient(apiUrl=girder_api_url)\n",
    "\n",
    "for i in range(n_files):\n",
    "    ultrasound_fullname = os.path.join(data_arrays_fullpath, training_ultrasound_filenames[i])\n",
    "    if not os.path.exists(ultrasound_fullname) or overwrite_existing_data_files:\n",
    "        print(\"Downloading {}...\".format(ultrasound_fullname))\n",
    "        gclient.downloadFile(training_ultrasound_ids[i], ultrasound_fullname)\n",
    "    f.value = i * 2 + 1\n",
    "    \n",
    "    segmentation_fullname = os.path.join(data_arrays_fullpath, training_segmentation_filenames[i])\n",
    "    if not os.path.exists(segmentation_fullname) or overwrite_existing_data_files:\n",
    "        print(\"Downloading {}...\".format(segmentation_fullname))\n",
    "        gclient.downloadFile(training_segmentation_ids[i], segmentation_fullname)\n",
    "    f.value = i * 2 + 2\n",
    "\n",
    "test_ultrasound_fullname = os.path.join(data_arrays_fullpath, testing_ultrasound_filename)\n",
    "if not os.path.exists(test_ultrasound_fullname):\n",
    "    print(\"Downloading {}...\".format(test_ultrasound_fullname))\n",
    "    gclient.downloadFile(testing_ultrasound_id, test_ultrasound_fullname)\n",
    "\n",
    "test_segmentation_fullname = os.path.join(data_arrays_fullpath, testing_segmentation_filename)\n",
    "if not os.path.exists(test_segmentation_fullname) or overwrite_existing_data_files:\n",
    "    print(\"Downloading {}...\".format(test_segmentation_fullname))\n",
    "    gclient.downloadFile(testing_segmentation_id, test_segmentation_fullname)\n",
    "    \n",
    "time_download_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal download time: {}\".format(time_download_stop - time_download_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acebaaa11bc84dbf87cf5f64fa4c17a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=16)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time to load from files: 0:00:00.113725\n"
     ]
    }
   ],
   "source": [
    "# Read data into numpy arrays\n",
    "\n",
    "ultrasound_arrays = []\n",
    "segmentation_arrays = []\n",
    "\n",
    "f = IntProgress(min=0, max=n_files * 2)\n",
    "display(f)\n",
    "\n",
    "time_start = datetime.datetime.now()\n",
    "\n",
    "for i in range(n_files):\n",
    "    ultrasound_fullname = os.path.join(data_arrays_fullpath, training_ultrasound_filenames[i])\n",
    "    segmentation_fullname = os.path.join(data_arrays_fullpath, training_segmentation_filenames[i])\n",
    "\n",
    "    ultrasound_data = np.load(ultrasound_fullname)\n",
    "    f.value = i * 2 + 1\n",
    "    \n",
    "    segmentation_data = np.load(segmentation_fullname)\n",
    "    f.value = i * 2 + 2\n",
    "    \n",
    "    ultrasound_arrays.append(ultrasound_data)\n",
    "    segmentation_arrays.append(segmentation_data)\n",
    "\n",
    "test_ultrasound_fullname = os.path.join(data_arrays_fullpath, testing_ultrasound_filename)\n",
    "test_ultrasound_array = np.load(test_ultrasound_fullname)\n",
    "\n",
    "test_segmentation_fullname = os.path.join(data_arrays_fullpath, testing_segmentation_filename)\n",
    "test_segmentation_array = np.load(test_segmentation_fullname)\n",
    "    \n",
    "time_stop = datetime.datetime.now()\n",
    "print(\"\\nTotal time to load from files: {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at: 2019-10-28 11:02:55.197562\n",
      "Number of epochs:    30\n",
      "Step size maximum:   0.02\n",
      "Step size decay:     0.0006663333333333333\n",
      "Batch size:          24\n",
      "Regularization rate: 0.0001\n",
      "\n",
      "*************************************  # 0\n",
      "\n",
      "Training on 2767 images, validating on 523 images.\n",
      "\n",
      "Metrics at the end of training\n",
      "  val_acc:   0.9818851569414595\n",
      "  val loss:  0.001836050091441329\n",
      "  val_dice:  0.5116926267543669\n",
      "  Total training time: 0:04:31.169057\n",
      "\n",
      "Validation predictions saved to: d:\\Data\\SagittalSpineSegmentationStudy\\PredictionsValidation\\q000_ultrasound_prediction.npy\n",
      "Testing predictions saved to:    d:\\Data\\SagittalSpineSegmentationStudy\\PredictionsTest\\prediction-test.npy\n",
      "Model saved to: d:\\Data\\SagittalSpineSegmentationStudy\\SavedModels\\SagittalSpineSegmentationStudy_model-0_2019-10-28_10-57-36.h5\n",
      "\n",
      "Total round time:     0:06:30.995072\n",
      "\n",
      "\n",
      "*************************************  # 1\n",
      "\n",
      "Training on 2935 images, validating on 355 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print training parameters, to archive them together with the notebook output.\n",
    "\n",
    "time_sequence_start = datetime.datetime.now()\n",
    "print(\"Training started at: {}\".format(time_sequence_start))\n",
    "print(\"Number of epochs:    {}\".format(num_epochs))\n",
    "print(\"Step size maximum:   {}\".format(max_learning_rate))\n",
    "print(\"Step size decay:     {}\".format(learning_rate_decay))\n",
    "print(\"Batch size:          {}\".format(batch_size))\n",
    "print(\"Regularization rate: {}\".format(regularization_rate))\n",
    "\n",
    "val_aurocs          = np.zeros(n_files)\n",
    "val_best_thresholds = np.zeros(n_files)\n",
    "val_best_tp         = np.zeros(n_files)\n",
    "val_best_fp         = np.zeros(n_files)\n",
    "val_recall          = np.zeros(n_files)\n",
    "val_precision       = np.zeros(n_files)\n",
    "val_fscore          = np.zeros(n_files)\n",
    "val_best_metrics    = dict()\n",
    "\n",
    "test_aurocs          = np.zeros(n_files)\n",
    "test_best_thresholds = np.zeros(n_files)\n",
    "test_best_tp         = np.zeros(n_files)\n",
    "test_best_fp         = np.zeros(n_files)\n",
    "test_recall          = np.zeros(n_files)\n",
    "test_precision       = np.zeros(n_files)\n",
    "test_fscore          = np.zeros(n_files)\n",
    "test_best_metrics    = dict()\n",
    "\n",
    "\n",
    "for i in range(n_files):\n",
    "    \n",
    "    # Prepare data arrays\n",
    "    \n",
    "    train_ultrasound_data = np.zeros(\n",
    "        [0, ultrasound_arrays[0].shape[1], ultrasound_arrays[0].shape[2], ultrasound_arrays[0].shape[3]])\n",
    "    train_segmentation_data = np.zeros(\n",
    "        [0, ultrasound_arrays[0].shape[1], ultrasound_arrays[0].shape[2], ultrasound_arrays[0].shape[3]])\n",
    "    \n",
    "    val_ultrasound_data = ultrasound_arrays[i]\n",
    "    val_segmentation_data = segmentation_arrays[i]\n",
    "    val_ultrasound_filename = training_ultrasound_filenames[i]\n",
    "    \n",
    "    for train_index in range(n_files):\n",
    "        if train_index != i:\n",
    "            train_ultrasound_data = np.concatenate((train_ultrasound_data, ultrasound_arrays[train_index]))\n",
    "            train_segmentation_data = np.concatenate((train_segmentation_data, segmentation_arrays[train_index]))\n",
    "    \n",
    "    n_train = train_ultrasound_data.shape[0]\n",
    "    n_val = val_ultrasound_data.shape[0]\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"*************************************  # {}\".format(i))\n",
    "    print(\"\")\n",
    "    print(\"Training on {} images, validating on {} images.\".format(n_train, n_val))\n",
    "    print(\"\")\n",
    "    \n",
    "    # Create and train model\n",
    "    \n",
    "    model = unet.sagittal_spine_unet(ultrasound_size, num_classes, regularization_rate)\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.adam(lr=max_learning_rate, decay=learning_rate_decay),\n",
    "              loss=[unet.weighted_categorical_crossentropy(WCE_weights)],\n",
    "              metrics=[\"accuracy\", unet.dice_coef])\n",
    "    \n",
    "    training_generator = generator.UltrasoundSegmentationBatchGenerator(\n",
    "        train_ultrasound_data,\n",
    "        train_segmentation_data[:, :, :, 0],\n",
    "        batch_size,\n",
    "        (ultrasound_size, ultrasound_size),\n",
    "        max_shift_factor=max_shift_factor,\n",
    "        min_zoom_factor=min_zoom_factor,\n",
    "        max_zoom_factor=max_zoom_factor,\n",
    "        max_rotation_angle=max_rotation_angle\n",
    "    )\n",
    "    \n",
    "    training_time_start = datetime.datetime.now()\n",
    "    \n",
    "    training_log = model.fit_generator(training_generator,\n",
    "                                       validation_data=(val_ultrasound_data, val_segmentation_data),\n",
    "                                       epochs=num_epochs,\n",
    "                                       verbose=0)\n",
    "        \n",
    "    training_time_stop = datetime.datetime.now()\n",
    "    \n",
    "    # Pring training log\n",
    "    \n",
    "    print(\"Metrics at the end of training\")\n",
    "    print(\"  val_acc:  \", training_log.history['val_acc'][-1])\n",
    "    print(\"  val loss: \", training_log.history['val_loss'][-1])\n",
    "    print(\"  val_dice: \", training_log.history['val_dice_coef'][-1])\n",
    "    print(\"  Total training time: {}\\n\".format(training_time_stop-training_time_start))\n",
    "    \n",
    "    # Plot training loss and metrics\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(training_log.history['loss'], 'bo--')\n",
    "    axes[0].plot(training_log.history['val_loss'], 'ro-')\n",
    "    axes[0].set(xlabel='Epochs (n)', ylabel='Loss')\n",
    "    axes[0].legend(['Training loss', 'Validation loss'])\n",
    "    \n",
    "    axes[1].plot(training_log.history['acc'], 'bo--')\n",
    "    axes[1].plot(training_log.history['val_acc'], 'ro-')\n",
    "    axes[1].set(xlabel='Epochs (n)', ylabel='Accuracy')\n",
    "    axes[1].legend(['Training accuracy', 'Validation accuracy'])\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # Predict on validation and test data\n",
    "    \n",
    "    y_pred_val  = model.predict(val_ultrasound_data)\n",
    "    y_pred_test = model.predict(test_ultrasound_array)\n",
    "    \n",
    "    # Saving L-1-O prediction for further evaluation\n",
    "    \n",
    "    filename_noext, extension = os.path.splitext(val_ultrasound_filename)\n",
    "    val_prediction_filename = filename_noext + \"_prediction.npy\"\n",
    "    val_prediction_fullname = os.path.join(val_data_fullpath, val_prediction_filename)\n",
    "    np.save(val_prediction_fullname, y_pred_val)\n",
    "    print(\"Validation predictions saved to: {}\".format(val_prediction_fullname))\n",
    "\n",
    "    # Saving test prediction for further evaluation\n",
    "    \n",
    "    test_prediction_filename = \"prediction-test.npy\"\n",
    "    test_prediction_fullname = os.path.join(test_data_fullpath, test_prediction_filename)\n",
    "    np.save(test_prediction_fullname, y_pred_test)\n",
    "    print(\"Testing predictions saved to:    {}\".format(test_prediction_fullname))\n",
    "    \n",
    "    # Archive trained model with unique filename based on timestamps\n",
    "    \n",
    "    model_file_name = this_notebook_name + \"_model-\" + str(i) + \"_\" + save_timestamp + \".h5\"\n",
    "    model_fullname = os.path.join(models_save_fullpath, model_file_name)\n",
    "    model.save(model_fullname)\n",
    "    print(\"Model saved to: {}\".format(model_fullname))\n",
    "    \n",
    "    # Validation output\n",
    "    \n",
    "    true_positives, false_positives, best_threshold_index, area, metrics_dicts = evaluation_metrics.compute_roc(\n",
    "        roc_thresholds, y_pred_val, val_segmentation_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    val_metrics = evaluation_metrics.compute_evaluation_metrics(\n",
    "        y_pred_val, val_segmentation_data, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    val_aurocs[i] = area\n",
    "    val_best_thresholds[i] = roc_thresholds[best_threshold_index]\n",
    "    val_best_tp[i] = true_positives[best_threshold_index]\n",
    "    val_best_fp[i] = false_positives[best_threshold_index]\n",
    "    val_best_metrics[i] = metrics_dicts[int(best_threshold_index)]\n",
    "    \n",
    "    val_recall[i] = val_metrics[evaluation_metrics.RECALL]\n",
    "    val_precision[i] = val_metrics[evaluation_metrics.PRECISION]\n",
    "    val_fscore[i] = val_metrics[evaluation_metrics.FSCORE]\n",
    "    \n",
    "    \n",
    "    # Test the model\n",
    "    \n",
    "    true_positives, false_positives, best_threshold_index, area, metrics_dicts = evaluation_metrics.compute_roc(\n",
    "        roc_thresholds, y_pred_test, test_segmentation_array, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    test_aurocs[i] = area\n",
    "    test_best_thresholds[i] = roc_thresholds[best_threshold_index]\n",
    "    test_best_tp[i] = true_positives[best_threshold_index]\n",
    "    test_best_fp[i] = false_positives[best_threshold_index]\n",
    "    test_best_metrics[i] = metrics_dicts[best_threshold_index]\n",
    "    \n",
    "    test_metrics = evaluation_metrics.compute_evaluation_metrics(\n",
    "        y_pred_test, test_segmentation_array, acceptable_margin_mm, mm_per_pixel)\n",
    "    \n",
    "    test_recall[i] = test_metrics[evaluation_metrics.RECALL]\n",
    "    test_precision[i] = test_metrics[evaluation_metrics.PRECISION]\n",
    "    test_fscore[i] = test_metrics[evaluation_metrics.FSCORE]\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Total round time:     {}\".format(datetime.datetime.now() - training_time_start))\n",
    "    print(\"\")\n",
    "    \n",
    "\n",
    "\n",
    "time_sequence_stop = datetime.datetime.now()\n",
    "\n",
    "print(\"\\nAll training stopped at: {}\".format(time_sequence_stop))\n",
    "print(\"\\nTotal training time:        {}\".format(time_sequence_stop - time_sequence_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange results in table\n",
    "\n",
    "results_df = pd.DataFrame(columns = [\n",
    "    \"Vali AUROC\",\n",
    "    \"Vali best thresh\",\n",
    "    \"Vali best TP\",\n",
    "    \"Vali best FP\",\n",
    "    \"Vali best recall\",\n",
    "    \"Vali best precis\",\n",
    "    \"Vali fuzzy recall\",\n",
    "    \"Vali fuzzy precis\",\n",
    "    \"Vali fuzzy Fscore\",\n",
    "    \"Test AUROC\",\n",
    "    \"Test best thresh\",\n",
    "    \"Test best TP\",\n",
    "    \"Test best FP\",\n",
    "    \"Test best recall\",\n",
    "    \"Test best precis\",\n",
    "    \"Test fuzzy recall\",\n",
    "    \"Test fuzzy precis\",\n",
    "    \"Test fuzzy Fscore\"\n",
    "])\n",
    "\n",
    "for i in range(n_files):\n",
    "    results_df.loc[i] = [\n",
    "        val_aurocs[i],\n",
    "        val_best_thresholds[i],\n",
    "        val_best_tp[i],\n",
    "        val_best_fp[i],\n",
    "        val_best_metrics[i][evaluation_metrics.RECALL],\n",
    "        val_best_metrics[i][evaluation_metrics.PRECISION],\n",
    "        val_recall[i],\n",
    "        val_precision[i],\n",
    "        val_fscore[i],\n",
    "        test_aurocs[i],\n",
    "        test_best_thresholds[i],\n",
    "        test_best_tp[i],\n",
    "        test_best_fp[i],\n",
    "        test_best_metrics[i][evaluation_metrics.RECALL],\n",
    "        test_best_metrics[i][evaluation_metrics.PRECISION],\n",
    "        test_recall[i],\n",
    "        test_precision[i],\n",
    "        test_fscore[i]\n",
    "    ]\n",
    "\n",
    "display(results_df)\n",
    "\n",
    "means_df = pd.DataFrame(results_df.mean(axis=0))\n",
    "\n",
    "print(\"\\nValidation averages\")\n",
    "display(means_df.T.loc[:, :\"Vali fuzzy Fscore\"])\n",
    "\n",
    "print(\"\\nTest averages\")\n",
    "display(means_df.T.loc[:, \"Test AUROC\":\"Test fuzzy Fscore\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results table\n",
    "\n",
    "csv_filename = this_notebook_name + \"_\" + save_timestamp + \".csv\"\n",
    "csv_fullname = os.path.join(results_save_fullpath, csv_filename)\n",
    "results_df.to_csv(csv_fullname)\n",
    "\n",
    "print(\"Results saved to: {}\".format(csv_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook so all output is archived by the next cell\n",
    "\n",
    "from IPython.display import Javascript\n",
    "script = '''\n",
    "require([\"base/js/namespace\"],function(Jupyter) {\n",
    "    Jupyter.notebook.save_checkpoint();\n",
    "});\n",
    "'''\n",
    "Javascript(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export HTML copy of this notebook\n",
    "\n",
    "notebook_file_name = this_notebook_name + \"_\" + save_timestamp + \".html\"\n",
    "notebook_fullname = os.path.join(notebooks_save_fullpath, notebook_file_name)\n",
    "\n",
    "os.system(\"jupyter nbconvert --to html \" + this_notebook_name + \" --output \" + notebook_fullname)\n",
    "print(\"Notebook saved to: {}\".format(notebook_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
