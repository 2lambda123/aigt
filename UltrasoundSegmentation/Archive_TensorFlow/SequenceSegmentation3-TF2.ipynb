{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trained tensorflow model\n",
    "\n",
    "model_fullname = r\"c:\\Data\\BreastSegmentation\\SavedModels\\BreastSegmentationStudy-TF2_model-2_2020-06-17_23-06-14.h5\"\n",
    "\n",
    "# Input ultrasound sequence names\n",
    "\n",
    "input_browser_name = r\"LumpNavRecording-20180508-104308_NeedleToProbe\"\n",
    "input_image_name = r\"LumpNavRecording-20180508-104308_NeedleToProbe-Image\"\n",
    "\n",
    "# Output will be saved using these names\n",
    "\n",
    "output_browser_name = r\"PredictionBrowser\"\n",
    "output_sequence_name = r\"PredictionSequence\"\n",
    "output_image_name = r\"PredictionImage\"\n",
    "\n",
    "# Optionally save output to numpy arrays\n",
    "\n",
    "array_output = False\n",
    "root_folder = \"c:\\Data\\BreastSegmentation\"\n",
    "array_folder_name = r\"Temp\"\n",
    "array_segmentation_name = r\"segmentation\"\n",
    "array_ultrasound_name = r\"ultrasound\"\n",
    "\n",
    "# Image processing parameters\n",
    "\n",
    "# Erases the side of prediction images. 1.0 means the whole prediction is erased.\n",
    "# Background should be the first component (i.e. y[:,:,:,0]) in the prediction output array.\n",
    "\n",
    "clip_side_ratio = 0.3\n",
    "apply_logarithmic_transformation = True\n",
    "logarithmic_transformation_decimals = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading model from: c:\\Data\\BreastSegmentation\\SavedModels\\BreastSegmentationStudy-TF2_model-2_2020-06-17_23-06-14.h5\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if keras model file exists. Abort if not found. Load model otherwise.\n",
    "\n",
    "if not os.path.exists(model_fullname):\n",
    "    raise Exception(\"Could not find model: \" + model_fullname)\n",
    "\n",
    "print(\"Loading model from: \" + model_fullname)\n",
    "\n",
    "if array_output:\n",
    "    array_output_fullpath = os.path.join(root_folder, array_folder_name)\n",
    "    array_segmentation_fullname = os.path.join(array_output_fullpath, array_segmentation_name)\n",
    "    array_ultrasound_fullname = os.path.join(array_output_fullpath, array_ultrasound_name)\n",
    "    if not os.path.exists(array_output_fullpath):\n",
    "        os.mkdir(array_output_fullpath)\n",
    "        print(\"Folder created: {}\".format(array_output_fullpath))\n",
    "    print(\"Will save segmentation output to {}\".format(array_segmentation_fullname))\n",
    "    print(\"Will save ultrasound output to   {}\".format(array_ultrasound_fullname))\n",
    "\n",
    "model = tf.keras.models.load_model(model_fullname, compile=False)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check input. Abort if browser or image doesn't exist.\n",
    "\n",
    "input_browser_node = slicer.util.getFirstNodeByName(input_browser_name, className='vtkMRMLSequenceBrowserNode')\n",
    "input_image_node = slicer.util.getFirstNodeByName(input_image_name, className=\"vtkMRMLScalarVolumeNode\")\n",
    "\n",
    "if input_browser_node is None:\n",
    "    logging.error(\"Could not find input browser node: {}\".format(input_browser_node))\n",
    "    raise\n",
    "\n",
    "if input_image_node is None:\n",
    "    logging.error(\"Could not find input image node: {}\".format(input_image_name))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create output image and browser for segmentation output.\n",
    "\n",
    "output_browser_node = slicer.util.getFirstNodeByName(output_browser_name, className='vtkMRMLSequenceBrowserNode')\n",
    "if output_browser_node is None:\n",
    "    output_browser_node = slicer.mrmlScene.AddNewNodeByClass('vtkMRMLSequenceBrowserNode', output_browser_name)\n",
    "\n",
    "output_sequence_node = slicer.util.getFirstNodeByName(output_sequence_name, className=\"vtkMRMLSequenceNode\")\n",
    "if output_sequence_node is None:\n",
    "    output_sequence_node = slicer.mrmlScene.AddNewNodeByClass('vtkMRMLSequenceNode', output_sequence_name)\n",
    "    output_browser_node.AddSynchronizedSequenceNode(output_sequence_node)\n",
    "\n",
    "output_image_node = slicer.util.getFirstNodeByName(output_image_name, className=\"vtkMRMLScalarVolumeNode\")\n",
    "if output_image_node is None:\n",
    "    volumes_logic = slicer.modules.volumes.logic()\n",
    "    output_image_node = volumes_logic.CloneVolume(slicer.mrmlScene, input_image_node, output_image_name)\n",
    "    browser_logic = slicer.modules.sequences.logic()\n",
    "    browser_logic.AddSynchronizedNode(output_sequence_node, output_image_node, output_browser_node)\n",
    "\n",
    "output_browser_node.SetRecording(output_sequence_node, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add all input sequences to the output browser for being able to conveniently replay everything\n",
    "\n",
    "proxy_collection = vtk.vtkCollection()\n",
    "input_browser_node.GetAllProxyNodes(proxy_collection)\n",
    "\n",
    "for i in range(proxy_collection.GetNumberOfItems()):\n",
    "    proxy_node = proxy_collection.GetItemAsObject(i)\n",
    "    output_sequence = slicer.mrmlScene.AddNewNodeByClass('vtkMRMLSequenceNode')\n",
    "    browser_logic.AddSynchronizedNode(output_sequence, proxy_node, output_browser_node)\n",
    "    output_browser_node.SetRecording(output_sequence, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Will segment 4520 images\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterate input sequence, compute segmentation for each frame, record output sequence.\n",
    "\n",
    "num_items = input_browser_node.GetNumberOfItems()\n",
    "n = num_items\n",
    "input_browser_node.SelectFirstItem()\n",
    "\n",
    "input_array = slicer.util.array(input_image_node.GetID())\n",
    "input_array_shape = input_array.shape\n",
    "\n",
    "print(\"Will segment {} images\".format(n))\n",
    "\n",
    "if array_output:\n",
    "    array_output_ultrasound = np.zeros((n, input_array.shape[0], input_array.shape[1]))\n",
    "    array_output_segmentation = np.zeros((n, input_array.shape[0], input_array.shape[1]), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Will mask 19 columns on both sides\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output_size = model.layers[-1].output_shape[1]\n",
    "num_output_components = model.layers[-1].output_shape[3]\n",
    "\n",
    "mask_model = np.ones([model_output_size, model_output_size])\n",
    "mask_model_background = np.zeros([model_output_size, model_output_size])\n",
    "\n",
    "columns_to_mask = int(model_output_size / 2 * clip_side_ratio)\n",
    "print(\"Will mask {} columns on both sides\".format(columns_to_mask))\n",
    "\n",
    "mask_model[:,:columns_to_mask] = 0\n",
    "mask_model[:,-columns_to_mask:] = 0\n",
    "mask_model_background[:,:columns_to_mask] = 1\n",
    "mask_model_background[:,-columns_to_mask:] = 1\n",
    "\n",
    "# Display mask\n",
    "\n",
    "# import matplotlib\n",
    "# matplotlib.use('WXAgg')\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# plt.imshow(mask_model[:,:])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing started at: 17-43-31\n",
       "4520\n",
       "Processing finished at: 17-46-28\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "start_timestamp = datetime.datetime.now()\n",
    "print(\"Processing started at: {}\".format(start_timestamp.strftime('%H-%M-%S')))\n",
    "\n",
    "print(n)\n",
    "for i in range(n):\n",
    "    # if i > 10:  # todo Just for debugging\n",
    "    #     break\n",
    "    input_array = slicer.util.array(input_image_node.GetID())\n",
    "    # print(\"input_array shape: {}\".format(input_array.shape))\n",
    "    \n",
    "    if array_output:\n",
    "        array_output_ultrasound[i, :, :] = input_array[0, :, :]\n",
    "    \n",
    "    resized_input_array = np.expand_dims(input_array, axis=3)\n",
    "    resized_input_array = tf.image.resize(resized_input_array[0, :, :, :], [model_output_size, model_output_size]).numpy()\n",
    "    resized_input_array = np.flip(resized_input_array, axis=0)\n",
    "    resized_input_array = resized_input_array / resized_input_array.max()  # Scaling intensity to 0-1\n",
    "    resized_input_array = np.expand_dims(resized_input_array, axis=0)\n",
    "\n",
    "    y = model.predict(resized_input_array)\n",
    "\n",
    "    if apply_logarithmic_transformation:\n",
    "        e = logarithmic_transformation_decimals\n",
    "        y = np.log10(np.clip(y, 10**(-e), 1.0)*(10**e))/e\n",
    "    y[0,:,:,:] = np.flip(y[0,:,:,:], axis=0)\n",
    "    \n",
    "    for component in range(1, num_output_components):\n",
    "        y[0,:,:,component] = y[0,:,:,component] * mask_model[:,:]\n",
    "    y[0,:,:,0] = np.maximum(y[0,:,:,0], mask_model_background)\n",
    "    \n",
    "    # print(\"target shape: {}\".format(input_array_shape))\n",
    "    upscaled_output_array = tf.image.resize(y[0,:,:,:], [input_array_shape[1], input_array_shape[2]])\n",
    "    # print(\"upscaled_output_array shape: {}\".format(upscaled_output_array.shape))\n",
    "    upscaled_output_array = upscaled_output_array[:,:,1] * 255\n",
    "    upscaled_output_array = np.clip(upscaled_output_array, 0, 255)\n",
    "    \n",
    "    if array_output:\n",
    "        array_output_segmentation[i, :, :] = upscaled_output_array[:, :].astype(np.uint8)\n",
    "    \n",
    "    # uncomment for debugging debugging\n",
    "    # im = Image.fromarray(upscaled_output_array)\n",
    "    # im = im.convert(\"L\")\n",
    "    # im.save(\"upscaled.jpeg\")\n",
    "    \n",
    "    # output_array = slicer.util.array(output_image_node.GetID())\n",
    "    # output_array[0, :, :] = upscaled_output_array[:, :].astype(np.uint8)\n",
    " \n",
    "    # print(\"upscaled_output_array shape: {}\".format(upscaled_output_array.shape))\n",
    "    slicer.util.updateVolumeFromArray(output_image_node, upscaled_output_array.astype(np.uint8)[np.newaxis, ...])\n",
    "    \n",
    "    output_browser_node.SaveProxyNodesState()\n",
    "    input_browser_node.SelectNextItem()\n",
    "    \n",
    "    # If Slicer crashes during processing, try commenting this following line out and run this notebook again.\n",
    "    slicer.app.processEvents()\n",
    "\n",
    "\n",
    "stop_timestamp = datetime.datetime.now()\n",
    "print(\"Processing finished at: {}\".format(stop_timestamp.strftime('%H-%M-%S')))\n",
    "\n",
    "\n",
    "if array_output:\n",
    "    np.save(array_ultrasound_fullname, array_output_ultrasound)\n",
    "    np.save(array_segmentation_fullname, array_output_segmentation)\n",
    "    print(\"Saved {}\".format(array_ultrasound_fullname))\n",
    "    print(\"Saved {}\".format(array_segmentation_fullname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processed 4520 frames in 177.13 seconds\n",
       "FPS = 25.52\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_seconds = (stop_timestamp - start_timestamp).total_seconds()\n",
    "print(\"Processed {} frames in {:.2f} seconds\".format(n, time_seconds))\n",
    "print(\"FPS = {:.2f}\".format(n / time_seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Slicer 4.11",
   "language": "python",
   "name": "slicer-4.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.13+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
